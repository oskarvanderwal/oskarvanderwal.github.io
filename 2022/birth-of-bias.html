<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- CSS file -->
    <link rel="stylesheet" href="https://unpkg.com/@picocss/pico@latest/css/pico.min.css">

    <!-- Script for counting website visitors -->
    <script async src="https://discreet-raccoon.pikapod.net/script.js" data-website-id="9238e917-e417-4d79-9832-6e02d5dd20cb"></script>

    <!-- Script for including feather icons: https://github.com/feathericons/feather -->
    <script src="https://unpkg.com/feather-icons"></script>
    <title>Oskar van der Wal's personal website</title>
</head>
<body>
    <nav class="container-fluid">
        <ul>
            <li><a href="https://odvanderwal.nl"><strong>Oskar van der Wal</strong></a></li>
        </ul>
        <ul>
            <li><a href="https://odvanderwal.nl/pages/publications.html" >Publications</a></li>
	    <li><a href="https://odvanderwal.nl/archives.html">Blog</a></li>
	    <li>
	      <details class="dropdown">
		<summary>
		  More
		</summary>
		<ul dir="rtl"> 
        	  <li><a href="https://scholar.google.nl/citations?user=AeHaYUoAAAAJ&hl=nl&oi=ao">Google Scholar</a></li>
        	  <li><a href="https://sigmoid.social/@oskarvanderwal">Mastodon</a></li>
		  <li><a href="https://bsky.app/profile/ovdw.bsky.social">Bluesky</a></li>
		  <li><a href="https://twitter.com/oskarvanderwal">Twitter</a></li>
      		</ul>
    	    </details>
            </li>
	</ul>
    </nav>

<article>
<main class="container">
<section id="content">
                <header>
                        <h1>
                                <a href=""
                                        rel="bookmark"
                                        title="Permalink to ðŸ“„ The Birth of Bias: A case study on the evolution of gender bias in an English language model">
                                        ðŸ“„ The Birth of Bias: A case study on the evolution of gender bias in an English language model
                                </a>
                        </h1>
		<dt><i>Thu 14 July 2022</i></dt>
                </header>
                </div>
                <p><p>Language models (LMs) have become essential building blocks of modern AI systems dealing with natural language. These models excel in diverse tasks including sentiment analysis, text generation, translation, and summarization. While their effectiveness stems from neural network architectures trained on vast datasets, this power comes with significant challenges.</p>
<p>Research has repeatedly shown that LMs can learn undesirable biases toward certain social groups, potentially leading to unfair decisions, recommendations, or text outputs. While detecting such biases is crucial, we must also understand their origins â€“ how these biases develop during training and what roles the training data, architecture, and downstream applications play throughout an NLP model's lifecycle.</p>
<p><strong>In this blog post, we share our research on the origins of bias in LMs by investigating the learning dynamics of an LSTM-based language model trained on an English Wikipedia corpus.</strong></p>
<h1>Gender Bias for Occupations in Language Models</h1>
<p>Our research focused on a relatively small LM based on the LSTM architecture <a href="https://arxiv.org/abs/1803.11138" title="K. Gulordava et al. &quot;Colorless Green Recurrent Networks Dream Hierarchically&quot;. 2018.">[1]</a>. We specifically examined gender bias â€“ one of the most studied types of bias in language models. This narrow focus allowed us to leverage existing methods for measuring gender bias and gain detailed insights into its development from the beginning of training â€“ an approach that wouldn't be feasible with very large pre-trained LMs.</p>
<p>The image below illustrates a typical language modeling pipeline. Gender bias can be measured at multiple stages:
- <strong>(I)</strong> In the training dataset
- <strong>(II)</strong> In the input embeddings (IE), which form the first layer of the LM
- <strong>(III)</strong> At the end of the pipeline in downstream tasks using the model's contextual embeddings</p>
<p><img alt="c685cc03a9e1997cf52e804659c48055.png" src="https://odvanderwal.nl/images/c685cc03a9e1997cf52e804659c48055.png"></p>
<p>We measured gender bias at each of these stages. It's worth noting that defining "undesirable bias" is complex, with many frameworks discussed in the literature [e.g. <a href="https://aclanthology.org/P19-1159.pdf" title="T. Sun et al. &quot;Mitigating Gender Bias in Natural Language Processing: Literature Review&quot;. 2019.">2</a>,<a href="https://aclanthology.org/2020.acl-main.468/" title="D. Shah et al. &quot;Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview&quot;. 2020.">3</a>,<a href="https://aclanthology.org/2020.acl-main.485/" title="S. L. Blodgett et al. &quot;Language (Technology) is Power: A Critical Survey of &quot;Bias&quot; in NLP&quot;. 2020.">4</a>]. In our work, we used gender-neutrality as the norm, defining bias as any significant deviation from a 50-50% distribution in preferences, probabilities, or similarities.</p>
<p>Following previous research [<a href="https://www.science.org/doi/abs/10.1126/science.aal4230" title="A. Caliskan et al. &quot;Semantics derived automatically from language corpora contain human-like biases&quot;. 2017.">5</a>,<a href="https://aclanthology.org/N18-2002.pdf" title="R. Rudinger et al. &quot;Gender Bias in Coreference Resolution&quot;. 2018.">6</a>,<a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020.">7</a>], we examined gender bias across 54 occupation terms. To quantify this bias, we used unambiguously gendered word pairs (e.g., "man"-"woman", "he"-"she", "king"-"queen") in our measurements. It's important to acknowledge that 'gender' is a multifaceted concept, far more complex than a simple male-female dichotomy [<a href="https://journals.sagepub.com/doi/abs/10.1177/0891243287001002002" title="C. West and D. H. Zimmerman. &quot;Doing gender&quot;. 1987">8</a>,<a href="https://www.tandfonline.com/doi/full/10.3109/09540261.2015.1106446" title="C. Richards et al. &quot;Non-binary or genderqueer genders&quot;. 2016.">9</a>].</p>
<h1>The Evolution of Gender Representation in Input Embeddings</h1>
<p>We first investigated how the LM learns to represent gender during training and how this representation becomes encoded in the input embeddings. To answer this question, we trained several linear classifiers to predict the gender of 82 gendered word pairs (e.g., "he"-"she", "son"-"daughter"). We analyzed snapshots at various points during training: after each epoch (one pass over all training data) and at multiple points during the first epoch. The LM was fully trained after 40 epochs.</p>
<p><img alt="embedding_space_decision_boundary.png" src="https://odvanderwal.nl/images/embedding_space_decision_boundary.png"></p>
<p>We tested these classifiers in two conditions:
1. Using all dimensions of the input embedding space
2. Using only the single most informative dimension (which we call the <strong>gender unit</strong>)</p>
<p>Interestingly, after just 2 epochs of training, using only the gender unit outperformed using all other dimensions of the input embedding space! This suggests that gender information becomes highly localized in the input embeddings early during training.</p>
<p><img alt="b0b3886708e79ac4530ae12296f8a358.png" src="https://odvanderwal.nl/images/b0b3886708e79ac4530ae12296f8a358.png"></p>
<p>Another fascinating discovery was that the dominant <strong>gender unit</strong> appears to specialize in representing female words, while information about male words remains distributed across the rest of the embedding space. The animation below illustrates this phenomenon:
- The top row shows correct predictions by the gender unit classifier
- The right column shows correct predictions by the classifier trained on all other dimensions
- Distance from the origin indicates how far a word is from the respective decision boundaries</p>
<p>Notice how words like "she", "her", "herself", and "feminine" quickly move to the top, while by the end of the animation, masculine words are only correctly predicted by the classifier that excludes the gender unit.</p>
<p><img alt="ezgif-7-ef259504a3.gif" src="https://odvanderwal.nl/images/ezgif-7-ef259504a3.gif"></p>
<h1>The Evolution of Gender Bias</h1>
<p>Using a linear decision boundary to study gender representation in embeddings aligns with one class of gender bias measures: those based on finding a linear <strong>gender subspace</strong>. Following Ravfogel, we define the gender subspace as the axis orthogonal to the decision boundary of a linear SVM trained to predict gender from a set of male and female words <a href="https://aclanthology.org/2020.acl-main.647/" title="S. Ravfogel et al. &quot;Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection&quot;. 2020.">[10]</a>.</p>
<p><img alt="embedding_space_gender_subspace.png" src="https://odvanderwal.nl/images/embedding_space_gender_subspace.png"></p>
<p>To determine a word's bias score in the embedding space, we measure its scalar projection onto the gender subspace. In the example below, words like "engineer" and "scientist" fall on the "male" side of the subspace (left of the purple decision boundary), while "nurse" and "receptionist" show female bias (right side). The further a word is from the decision boundary, the stronger its bias.</p>
<p><img alt="embedding_space_occupation_bias.png" src="https://odvanderwal.nl/images/embedding_space_occupation_bias.png"></p>
<p>While this gives us a measure of gender bias in the LM's input embeddings, it's important to remember that this represents only one part of the model. The bias that actually affects users appears in downstream tasks, not in the hidden input embeddings. Unfortunately, the relationship between a model's internal bias and its downstream behavior isn't straightforward.</p>
<p>For this reason, we also measured gender bias in a downstream task â€“ specifically, the <em>Semantic Textual Similarity task for Bias</em> (<strong>STS-B</strong>) <a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020.">[11]</a>. In this task, the LM estimates the similarity between three sentences containing either "man," "woman," or an occupation term. The gender bias for that occupation is the difference in similarity scores averaged across 276 template sentences.</p>
<p>The example below shows how we calculated bias for "janitor" using one template sentence, resulting in a score of 0.75 - 0.54 = 0.21 (male bias).</p>
<p><img alt="STS-B.png" src="https://odvanderwal.nl/images/STS-B.png"></p>
<p>Using these measurement tools, we tracked how gender bias in both input embeddings and the STS-B task evolved during training. We found significant gender bias in both representations (which correlated with actual gender distributions in US Labor Statistics <a href="https://www.science.org/doi/abs/10.1126/science.aal4230" title="A. Caliskan et al. &quot;Semantics derived automatically from language corpora contain human-like biases&quot;. 2017.">[5]</a>).</p>
<p>The figure below shows bias scores at three points during training. The bias measured in input embeddings reasonably predicted the bias in the STS-B task, although the correlation between these measures reached only 0.6 by the end of training â€“ indicating important differences remain. One notable difference was a gender asymmetry in the input embedding bias (skewed toward female bias at epoch 40) that didn't appear in the downstream task.</p>
<p><img alt="f32e52d4a61f0785298db563c2c5d8ca.png" src="https://odvanderwal.nl/images/f32e52d4a61f0785298db563c2c5d8ca.png"></p>
<h1>Diagnostic Intervention: Changing Downstream Bias by Modifying Embeddings</h1>
<p>While our findings showed correlation between input embedding bias and STS-B bias, we wanted to determine if there was a causal relationship. We conducted a diagnostic intervention to answer the question: <em>Does debiasing the input embeddings measurably affect downstream bias?</em></p>
<p>Again following Ravfogel's method <a href="https://aclanthology.org/2020.acl-main.647/" title="S. Ravfogel et al. &quot;Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection&quot;. 2020.">[10]</a>, we debiased the input embeddings by projecting all words onto the null-space of the gender subspace (the decision boundary), effectively removing gender information from this subspace. Since there may be multiple linear representations of gender, this procedure should be repeated several times. We indicate the number of debiasing iterations with <strong>k</strong>.</p>
<p><img alt="embedding_space_debiased.png" src="https://odvanderwal.nl/images/embedding_space_debiased.png"></p>
<p>To study how debiasing input embeddings affects downstream STS-B task bias, we performed 10 debiasing iterations. For each step <strong>k</strong>, we measured:
1. The STS-B bias
2. The effect on the model's <a href="https://en.wikipedia.org/wiki/Perplexity" title="An increase in perplexity means the LM becomes worse at language modeling.">perplexity</a>
3. The <a href="https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full" title="We used Representational Similarity Analysis (RSA) to measure changes in embedding space topology due to debiasing.">topological similarity</a> of the embeddings compared to the original model</p>
<p>The figure below shows results for the LM after 1 and 40 epochs of training. We observed several surprising patterns:</p>
<ol>
<li>
<p>While the original LM had higher bias at epoch 40, debiasing was much more effective at this later stage, ultimately resulting in lower bias. This aligns with our earlier finding that gender information becomes more localized over training, making it easier to remove selectively.</p>
</li>
<li>
<p>While extensive debiasing often damages the LM, applying just three debiasing iterations reduced bias without significant side effects. Both perplexity and embedding topological similarity remained close to pre-debiasing values.</p>
</li>
</ol>
<p><img alt="8b3a882ec09f00a023c0ea0ee781eb95.png" src="https://odvanderwal.nl/images/8b3a882ec09f00a023c0ea0ee781eb95.png"></p>
<p>Given the gender asymmetries we observed earlier, we also analyzed debiasing effects on female-biased and male-biased occupations separately. The figure below shows the impact on input embedding bias (left) and STS-B bias (right). Interestingly, a single debiasing iteration primarily reduced female bias in input embeddings (and even increased measured male bias) â€“ possibly explained by the dominant gender unit's specialization in female word information. </p>
<p>Surprisingly, for STS-B bias, the first debiasing step reduced male bias more significantly. This discrepancy requires further research.</p>
<p><img alt="e8240b159bf9bd220fc6a9ec11716371.png" src="https://odvanderwal.nl/images/e8240b159bf9bd220fc6a9ec11716371.png"></p>
<p>While we confirmed a causal relationship between input embedding bias and STS-B bias, more research is needed to understand their precise connection. It's particularly important to consider potential gender asymmetries in the model's internal representations, as naive mitigation strategies might introduce new biases through disparate effects on different social categories.</p>
<h1>Conclusions</h1>
<p>Our research revealed several key insights about gender bias in language models:</p>
<ul>
<li>
<p>We identified distinct phases in the learning dynamics of gender representation and bias, characterized by changes in how locally gender information is represented and which dataset features explain these changes.</p>
</li>
<li>
<p>Gender becomes increasingly locally encoded in input embeddings during training, with the dominant <strong>gender unit</strong> specializing in representing feminine word information.</p>
</li>
<li>
<p>Bias in input embeddings reasonably predicts downstream bias. Debiasing input embeddings effectively reduces downstream bias, indicating a causal relationship.</p>
</li>
<li>
<p>We observed striking gender asymmetries, including the gender unit's specialization in female information and different debiasing effects depending on a word's gender bias. This underscores the importance of understanding how gender bias is represented within LMs, as naive debiasing approaches may inadvertently introduce new biases.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="nv">@article</span><span class="err">{</span><span class="n">van2022birth</span><span class="p">,</span>
<span class="w">  </span><span class="n">title</span><span class="o">=</span><span class="err">{</span><span class="n">The</span><span class="w"> </span><span class="n">Birth</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="nl">Bias</span><span class="p">:</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="k">case</span><span class="w"> </span><span class="n">study</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">evolution</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">gender</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">English</span><span class="w"> </span><span class="k">language</span><span class="w"> </span><span class="n">model</span><span class="err">}</span><span class="p">,</span>
<span class="w">  </span><span class="n">author</span><span class="o">=</span><span class="err">{</span><span class="n">van</span><span class="w"> </span><span class="n">der</span><span class="w"> </span><span class="n">Wal</span><span class="p">,</span><span class="w"> </span><span class="n">Oskar</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Jumelet</span><span class="p">,</span><span class="w"> </span><span class="n">Jaap</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Schulz</span><span class="p">,</span><span class="w"> </span><span class="n">Katrin</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">Zuidema</span><span class="p">,</span><span class="w"> </span><span class="n">Willem</span><span class="err">}</span><span class="p">,</span>
<span class="w">  </span><span class="n">journal</span><span class="o">=</span><span class="err">{</span><span class="n">arXiv</span><span class="w"> </span><span class="n">preprint</span><span class="w"> </span><span class="nl">arXiv</span><span class="p">:</span><span class="mf">2207.10245</span><span class="err">}</span><span class="p">,</span>
<span class="w">  </span><span class="nf">year</span><span class="o">=</span><span class="err">{</span><span class="mi">2022</span><span class="err">}</span>
<span class="err">}</span>
</code></pre></div></p>
</section>
</main>
</article>

</body>
</html>