<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- CSS file -->
    <link rel="stylesheet" href="https://unpkg.com/@picocss/pico@latest/css/pico.min.css">

    <!-- Script for counting website visitors -->
    <script async src="https://discreet-raccoon.pikapod.net/script.js" data-website-id="9238e917-e417-4d79-9832-6e02d5dd20cb"></script>

    <!-- Script for including feather icons: https://github.com/feathericons/feather -->
    <script src="https://unpkg.com/feather-icons"></script>
    <title>Oskar van der Wal's personal website</title>
</head>
<body>
    <nav class="container-fluid">
        <ul>
            <li><a href="https://odvanderwal.nl"><strong>Oskar van der Wal</strong></a></li>
        </ul>
        <ul>
            <li><a href="https://odvanderwal.nl/pages/publications.html" >Publications</a></li>
	    <li><a href="https://odvanderwal.nl/archives.html">Blog</a></li>
	    <li>
	      <details class="dropdown">
		<summary>
		  More
		</summary>
		<ul dir="rtl"> 
        	  <li><a href="https://scholar.google.nl/citations?user=AeHaYUoAAAAJ&hl=nl&oi=ao">Google Scholar</a></li>
        	  <li><a href="https://sigmoid.social/@oskarvanderwal">Mastodon</a></li>
		  <li><a href="https://bsky.app/profile/ovdw.bsky.social">Bluesky</a></li>
		  <li><a href="https://twitter.com/oskarvanderwal">Twitter</a></li>
      		</ul>
    	    </details>
            </li>
	</ul>
    </nav>

<article>
<main class="container">
<section id="content">
                <header>
                        <h1>
                                <a href=""
                                        rel="bookmark"
                                        title="Permalink to Taking a step back and positioning bias: three considerations">
                                        Taking a step back and positioning bias: three considerations
                                </a>
                        </h1>
		<dt><i>Sun 01 January 2023</i></dt>
                </header>
                </div>
                <p><p>In a <a href="https://odvanderwal.nl/2022/gender-bias-in-language-models.html">previous blog post</a>, I have mentioned some ways to investigate the gender bias of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> (NLP) systems (language models). When discussing undesirable biases, we often take a mathematical and 'mechanistic' approach, measuring deviations from a prescriptive norm of ideal behavior (e.g., a skew in gender distribution from 50/50%) or trying to explain how biases are encoded in the NLP model's parameters.</p>
<p>However, it is valuable to take a step back and consider bias in NLP from a broader perspective. The analysis of bias is incomplete if we ignore normative questions and the sociotechnical context; both the technical details of the model and social aspects (designers, users, stakeholders, historical and cultural context, company goals, etc.) are important to consider. In this blog post, we'll discuss three key considerations:</p>
<ol>
<li>Algorithmic bias is a sociotechnical problem</li>
<li>Society is constantly changing and so is our conceptualization of bias</li>
<li>Algorithmic bias is not <em>simply</em> a reflection of the data/society</li>
</ol>
<h1>1. Bias is a sociotechnical problem</h1>
<p>When should we consider the gender bias of an AI system harmful? The implicit assumption in AI debates is generally that we should aim for gender-neutral behavior, based on the idea that not differentiating between genders constitutes fair behavior. However, whether this is true might depend strongly on the particular task we want the system to perform and its sociotechnical context (i.e., both technical and social aspects matter).</p>
<p>In translations, we might want the AI system to consider the (grammatical) gender of the subject, but not in <a href="https://www.siliconrepublic.com/careers/amazon-ai-hiring-tool-women-discrimination">assessing the competency of job candidates when automatically filtering resumes</a>! (In fact, whether we should use AI for automating these tasks is another question entirely.)</p>
<p>Our perspective may shift if we view AI bias not in isolation, but as situated within broader practices: Individual examples of bias may not reveal the full picture of <a href="https://ung.edu/diversity/bias.php">structural bias</a> within institutions, businesses, or organizations using these systems. Why does an AI system assign higher competency scores to resumes of people similar to existing employees? Is it because they are truly more competent, or is the training dataset skewed due to historical reasons, and would more diversity actually benefit the company? In this light, we might even need to consider adding counteracting bias to generate equal opportunities for different subgroups, compensating for disadvantages these groups face.</p>
<p>Not all bias is unwanted, and there might be contexts where it's necessary to reach certain goals. To formulate appropriate (moral) standards for an AI system, we need to:
- Examine the broader context in which it functions
- Understand how the AI system interacts with its environment
- Consider how the entire system might contribute to unfairness or harm particular groups</p>
<p>This suggests the current paradigm for analyzing bias in NLP may be inadequate: <a href="https://arxiv.org/abs/2111.15366">Raji et al. (2021)</a> make a compelling argument that benchmarks for evaluating AI systems are fundamentally limited, as they consist of decontextualized examples.</p>
<h1>2. Society is constantly changing and so is "bias"</h1>
<p>Ideally, discussions about norms and standards for a particular AI application would be resolved before development begins. But what counts as unfair or harmful behavior isn't stable—these factors evolve as societal debates progress, making a definitive solution to bias impossible. Worse, new biases can emerge if our AI systems don't adjust to these changes (<a href="https://doi.org/10.1145/230538.230561">Friedman and Nissenbaum, 1996</a>; <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">Bender et al., 2021</a>). This concern is especially relevant for very large language models, which are expensive to train and therefore reused for many downstream tasks (<a href="https://dl.acm.org/doi/10.1145/3442188.3445922">Bender et al., 2021</a>).</p>
<p>Moreover, given the diverse applications that could use language technology, no single set of standards can fit them all. However, we can be transparent and detailed about how a particular model is trained, including dataset information, making this available for model transfer scenarios (<a href="https://doi.org/10.1145/3287560.3287596">Mitchell et al., 2019</a>; <a href="https://www.aclweb.org/anthology/2020.acl-main.463">Bender and Koller, 2020</a>; <a href="https://dl.acm.org/doi/fullHtml/10.1145/3458723?casa_token=TU9HFtTyB88AAAAA:J97L_b3qNCzO-r8MLt2yVnE9D6P4LwXsbuJDlhbUfYM1aTVT9oduXJq-reWE_zWaFw5DtrQyl-gH">Gebru et al., 2021</a>; <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">Bender et al., 2021</a>). Furthermore, we need technologies that allow us to counteract biases whenever they matter for downstream tasks. For this, we need a clear understanding of how bias originates in the first place.</p>
<h1>3. Algorithmic bias is not only reflecting pre-existing bias</h1>
<p>A popular argument in the AI community is that deep neural model bias simply reflects pre-existing biases in training data. However, we shouldn't neglect our responsibility in designing and implementing these systems: Many forms of bias can emerge at different stages of creating and deploying language technology (see <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432">Hovy and Prabhumoye, 2021</a>). Others have pointed out that biased algorithms can transform society in profound ways. For example, <a href="https://proceedings.mlr.press/v81/ensign18a.html">Ensign et al. (2018)</a> show how biased policing algorithms could result in increased surveillance of certain neighborhoods, which then feeds back into new data reinforcing the earlier bias, creating a 'runaway feedback loop'.</p>
<p>Language technology doesn't merely reflect society—its implementations become part of society and can reshape it in unexpected ways. A well-known theme in the <a href="https://www.futurelearn.com/info/courses/philosophy-of-technology/0/steps/26311">philosophy of technology</a> is that technologies 'mediate' our experiences and shape our worldview of "how to live" (<a href="https://www.psupress.org/books/titles/0-271-02539-5.html">Verbeek, 2005</a>). Machine translation systems may promote a worldview primarily of men, with women restricted to stereotypical occupations (<a href="https://www.humanamente.eu/index.php/HM/article/view/307">Wellner, 2020</a>), and search engines showing only men for "CEO" searches similarly shape our perception of the archetypal business leader. Consider this example from <a href="https://unesdoc.unesco.org/ark:/48223/pf0000367823">this UNESCO/COMEST report from 2019</a>:</p>
<blockquote>
<p>"The 'gendering' of digital assistants, for example, may reinforce understandings of women as subservient and compliant. Indeed, female voices are routinely chosen as personal assistance bots, mainly fulfilling customer service duties, whilst the majority of bots in professional services such as the law and finance sectors, for example, are coded as male voices. This has educational implications with regards to how we understand 'male' vs 'female' competences, and how we define authoritative versus subservient positions."</p>
</blockquote>
<p>How we define and measure bias may also influence how we conceptualize bias itself. In fairness metrics discussions, Jacobs and Wallach (<a href="https://doi.org/10.1145/3442188.3445901">2021</a>) highlight 'consequential validity'—the fact that "measurements shape the ways that we understand the construct itself"—which is often overlooked when designing bias metrics.</p>
<p>How we define and measure racial and gender categorizations shapes how we view and act on these constructs in society; viewing gender as binary may harm non-binary communities (<a href="https://papers.ssrn.com/abstract=3189696">Costanza-Chock, 2018</a>). (And see <a href="https://books.google.nl/books?hl=nl&amp;lr=&amp;id=BguXDwAAQBAJ&amp;oi=fnd&amp;pg=PP1&amp;dq=What+Is+Race%3F+glasgow&amp;ots=sdLkgr5WNm&amp;sig=le0lOZEXn-WqjaEAyq1E2EHCoqI#v=onepage&amp;q=What%20Is%20Race%3F%20glasgow&amp;f=false">Glasgow, 2019</a> for a discussion of different perspectives on <em>race</em>.)</p>
<p>Algorithmic bias is inherently complex due to its sociotechnical and context-sensitive nature, making precise definition difficult—yet discussions about its conceptualization are crucial for research (e.g., <a href="https://www.aclweb.org/anthology/2020.acl-main.485">Blodgett et al., 2020</a>, <a href="https://arxiv.org/abs/2211.13709">Van der Wal et al. 2024</a>). Researchers cannot rely on a 'catch-all' bias metric, and mitigating harms might require more than simply removing biased information (<a href="https://aclanthology.org/2022.bigscience-1.3">Talat et al., 2022</a>). Is completely debiasing an AI system even possible? (See <a href="https://arxiv.org/abs/2101.11974">Talat et al., 2021</a> for a discussion of <em>debiasing</em>.)</p>
<h1>Looking Forward</h1>
<p>Perhaps our starting point should not be how to debias AI models, as suggested in <a href="https://kvab.be/sites/default/rest/blobs/2806/tw_waardevoldigitaliseren.pdf">this Dutch report on digitalization</a>, but rather to focus on larger societal questions: How do we want to shape the world with language technology as part of life? How can we design AI systems that help create a more just society, instead of reinforcing or creating new forms of systemic bias? </p>
<p>Such broad discussions about fair behavior in AI systems need to involve not only AI researchers but various experts from outside the technical domain. By expanding the conversation beyond technical solutions to encompass ethical, social, and philosophical dimensions, we can work toward language technologies that truly serve the diverse needs of our changing society.</p>
<hr>
<p>Thanks to <a href="https://staff.fnwi.uva.nl/w.c.moltmaker/">Wout Moltmaker</a> for his helpful comments on this blog post.</p></p>
</section>
</main>
</article>

</body>
</html>