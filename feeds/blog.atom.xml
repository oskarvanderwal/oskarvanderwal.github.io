<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Oskar van der Wal - blog</title><link href="https://odvanderwal.nl/" rel="alternate"></link><link href="https://odvanderwal.nl/feeds/blog.atom.xml" rel="self"></link><id>https://odvanderwal.nl/</id><updated>2024-01-24T00:00:00+01:00</updated><entry><title>Paper: "Undesirable Biases in NLP: Addressing Challenges of Measurement"</title><link href="https://odvanderwal.nl/2024/paper-challenges-of-measurement.html" rel="alternate"></link><published>2024-01-24T00:00:00+01:00</published><updated>2024-01-24T00:00:00+01:00</updated><author><name>Oskar van der Wal</name></author><id>tag:odvanderwal.nl,2024-01-24:/2024/paper-challenges-of-measurement.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This post is about our paper &lt;a href="https://doi.org/10.1613/jair.1.15195"&gt;"Undesirable Biases in NLP: Addressing Challenges of Measurement"&lt;/a&gt;, published in JAIR.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Developing tools for measuring &amp;amp; mitigating bias is challenging: LM bias is a complex sociocultural phenomenon and we have no access to a ground truth. We voice our concerns about current bias evaluation practices …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This post is about our paper &lt;a href="https://doi.org/10.1613/jair.1.15195"&gt;"Undesirable Biases in NLP: Addressing Challenges of Measurement"&lt;/a&gt;, published in JAIR.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Developing tools for measuring &amp;amp; mitigating bias is challenging: LM bias is a complex sociocultural phenomenon and we have no access to a ground truth. We voice our concerns about current bias evaluation practices, and discuss how we can ensure the quality of bias measures despite these challenges.&lt;/p&gt;
&lt;h2&gt;Operationalizations of Bias&lt;/h2&gt;
&lt;p&gt;Borrowing from psychometrics (a field specialized in the measurement of concepts that are not directly observable), we argue that it is useful to decouple the "construct" (what we want to know about but cannot observe directly) from its "operationalization" (the imperfect proxy).&lt;/p&gt;
&lt;p&gt;&lt;img alt="bias_operationalizations.png" src="https://odvanderwal.nl/images/bias_operationalizations.png"&gt;&lt;/p&gt;
&lt;p&gt;It's important to understand the difference! For instance, a twice-as-high bias score (operationalization) does not necessarily mean that the model is twice as biased (construct). Making this distinction allows us to be more explicit about our assumptions and conceptualizations.&lt;/p&gt;
&lt;p&gt;We discuss two important concepts that say something about the quality of bias measures: &lt;strong&gt;reliability&lt;/strong&gt; and &lt;strong&gt;construct validity&lt;/strong&gt;. For both, we discuss strategies for how to assess these in the NLP setting.&lt;/p&gt;
&lt;h2&gt;Reliability&lt;/h2&gt;
&lt;p&gt;How much precision can we get when applying the bias measure? How resilient is it to random measurement error? Naturally, we prefer measurement tools with a higher reliability! In our paper we discuss four forms of reliability we think can be applied easily to the NLP context.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Reliability type&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Consistency across&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Example application&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Inter-rater&lt;/td&gt;
&lt;td&gt;(Human) annotators&lt;/td&gt;
&lt;td&gt;Annotating potential test items&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Internal consistency&lt;/td&gt;
&lt;td&gt;Test items of a measure&lt;/td&gt;
&lt;td&gt;Templates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Parallel-form&lt;/td&gt;
&lt;td&gt;Alternative versions of a measure&lt;/td&gt;
&lt;td&gt;Bias benchmarks &amp;amp; prompts&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Seed-based test-retest&lt;/td&gt;
&lt;td&gt;Random seeds&lt;/td&gt;
&lt;td&gt;Model retraining&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Corpus-based test-retest&lt;/td&gt;
&lt;td&gt;Training data sets&lt;/td&gt;
&lt;td&gt;Model retraining&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Time-based test-retest&lt;/td&gt;
&lt;td&gt;Time&lt;/td&gt;
&lt;td&gt;Training steps &amp;amp; temporal data&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For instance, parallel-form reliability tests if different—but designed to be equivalent—versions of a measure are consistent. E.g., how consistent are different prompt formulations for evaluating the LM responses on the same bias dataset? Are LMs sensitive to minor changes to how the questions are phrased?&lt;/p&gt;
&lt;p&gt;Relatedly, &lt;a href="https://bsky.app/profile/lchoshen.bsky.social/post/3k66c6hjqr623"&gt;Leshem Choshen and his co-authors&lt;/a&gt; comes with a compelling argument: not the size of a benchmark, but its reliability matters! We waste valuable resources when computing results for test items that do not contribute to the overall reliability of the dataset.&lt;/p&gt;
&lt;h2&gt;Construct Validity&lt;/h2&gt;
&lt;p&gt;How sure are we that we measure what we actually want to measure (the construct)? Critical work by e.g., Gonen &amp;amp; Goldberg, Blodgett et al., Orgad &amp;amp; Belinkov shows many flaws that could hurt the validity. How do we design bias measures that actually measure what we want?&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Validity type&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Focus&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Convergent:&lt;/strong&gt; Do measurements from this instrument relate to measures that they should relate to?&lt;/td&gt;
&lt;td&gt;related measure or construct&lt;/td&gt;
&lt;td&gt;downstream harm&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Divergent:&lt;/strong&gt; Do measurements from this instrument not relate (or only relate weakly) to measures that they should not relate (or only relate weakly) to?&lt;/td&gt;
&lt;td&gt;confounding construct&lt;/td&gt;
&lt;td&gt;general model capability&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Content:&lt;/strong&gt; Are all relevant sub-components of the construct represented sufficiently by measures from   this instrument? Is none of the instrument's materials construct( subcomponent)-irrelevant?&lt;/td&gt;
&lt;td&gt;relevant subcomponents of the construct&lt;/td&gt;
&lt;td&gt;different forms of gender bias&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ideally, one would test the validity by comparing one’s bias results with a gold standard (criterion validity). Unfortunately, we do not have access to this for something like model bias. However, there exist alternative (weaker) strategies for validating bias benchmarks!&lt;/p&gt;
&lt;h3&gt;Convergent and Divergent Validity&lt;/h3&gt;
&lt;p&gt;An obvious approach is to test the &lt;strong&gt;convergent validity&lt;/strong&gt;: How well do our bias scores relate to other bias measures? And—more importantly—to the downstream harms of LMs? (Sometimes called predictive validity) See, for example, the work by Delobelle et al., Goldfarb-Tarrant et al., and others.&lt;/p&gt;
&lt;p&gt;&lt;img alt="convergent_divergent_validity.png" src="https://odvanderwal.nl/images/convergent_divergent_validity.png"&gt;&lt;/p&gt;
&lt;p&gt;However, we believe that its flip side, &lt;strong&gt;divergent validity&lt;/strong&gt;, deserves attention as well! Instead, we ask whether the bias measure is not too similar to another (easily confounded) measure or construct. We do not want to accidentally also measure something else.&lt;/p&gt;
&lt;p&gt;Maybe you want to decouple gender bias from grammatical gender (see e.g., Limisiewicz &amp;amp; Mareček). Or—when comparing the bias of models of different sizes—to make sure that model capability is not a confounding factor. Maybe smaller models do not respond well to prompting and appear to be less biased, while with simpler tasks they may show more bias.&lt;/p&gt;
&lt;p&gt;If you use/develop bias measures, we encourage you to apply a psychometric lens for reliably measuring the construct of interest. We end our paper with guidelines for developing bias measurement tools, which complements excellent advice by Dev et al., Talat et al., Blodgett et al. and others!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;@article&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;vanderwal2024undesirable&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;Undesirable&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;biases&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;NLP&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Addressing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;challenges&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;measurement&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;author&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;van&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;der&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Wal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Oskar&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Bachmann&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dominik&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Leidinger&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Alina&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;van&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Maanen&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Leendert&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Zuidema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Willem&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Schulz&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Katrin&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;journal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="n"&gt;Journal&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Artificial&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Intelligence&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Research&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;volume&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;79&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;pages&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nf"&gt;year&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="mi"&gt;2024&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content><category term="blog"></category></entry><entry><title>Taking a step back and positioning bias: three considerations</title><link href="https://odvanderwal.nl/2023/positioning-bias.html" rel="alternate"></link><published>2023-01-01T00:00:00+01:00</published><updated>2023-01-01T00:00:00+01:00</updated><author><name>Oskar van der Wal</name></author><id>tag:odvanderwal.nl,2023-01-01:/2023/positioning-bias.html</id><summary type="html">&lt;p&gt;In my research, I use various approaches to investigate social bias in language models. 
When discussing such undesirable biases, we often take a mathematical and 'mechanistic' approach, measuring deviations from a prescriptive norm of ideal behavior (e.g., a skew in gender distribution from 50/50%) or trying to explain …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my research, I use various approaches to investigate social bias in language models. 
When discussing such undesirable biases, we often take a mathematical and 'mechanistic' approach, measuring deviations from a prescriptive norm of ideal behavior (e.g., a skew in gender distribution from 50/50%) or trying to explain how biases are encoded in the NLP model's parameters.&lt;/p&gt;
&lt;p&gt;However, it is valuable to sometimes take a step back and consider bias in NLP from a broader perspective. The analysis of bias is incomplete if we ignore normative questions and the sociotechnical context; both the technical details of the model and social aspects (designers, users, stakeholders, historical and cultural context, company goals, etc.) are important to consider. In this blog post, we'll discuss three key considerations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Algorithmic bias is a sociotechnical problem&lt;/li&gt;
&lt;li&gt;Society is constantly changing and so is our conceptualization of bias&lt;/li&gt;
&lt;li&gt;Algorithmic bias is not &lt;em&gt;simply&lt;/em&gt; a reflection of the data/society&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;1. Bias is a sociotechnical problem&lt;/h1&gt;
&lt;p&gt;When should we consider the gender bias of an AI system harmful? The implicit assumption in AI debates is generally that we should aim for gender-neutral behavior, based on the idea that not differentiating between genders constitutes fair behavior. However, whether this is true might depend strongly on the particular task we want the system to perform and its sociotechnical context (i.e., both technical and social aspects matter).&lt;/p&gt;
&lt;p&gt;In translations, we might want the AI system to consider the (grammatical) gender of the subject, but not in &lt;a href="https://www.siliconrepublic.com/careers/amazon-ai-hiring-tool-women-discrimination"&gt;assessing the competency of job candidates when automatically filtering resumes&lt;/a&gt;! (In fact, whether we should use AI for automating these tasks is another question entirely.)&lt;/p&gt;
&lt;p&gt;Our perspective may shift if we view AI bias not in isolation, but as situated within broader practices: Individual examples of bias may not reveal the full picture of &lt;a href="https://ung.edu/diversity/bias.php"&gt;structural bias&lt;/a&gt; within institutions, businesses, or organizations using these systems. Why does an AI system assign higher competency scores to resumes of people similar to existing employees? Is it because they are truly more competent, or is the training dataset skewed due to historical reasons, and would more diversity actually benefit the company? In this light, we might even need to consider adding counteracting bias to generate equal opportunities for different subgroups, compensating for disadvantages these groups face.&lt;/p&gt;
&lt;p&gt;Not all bias is unwanted, and there might be contexts where it's necessary to reach certain goals. To formulate appropriate (moral) standards for an AI system, we need to:
- Examine the broader context in which it functions
- Understand how the AI system interacts with its environment
- Consider how the entire system might contribute to unfairness or harm particular groups&lt;/p&gt;
&lt;p&gt;This suggests the current paradigm for analyzing bias in NLP may be inadequate: &lt;a href="https://arxiv.org/abs/2111.15366"&gt;Raji et al. (2021)&lt;/a&gt; make a compelling argument that benchmarks for evaluating AI systems are fundamentally limited, as they consist of decontextualized examples.&lt;/p&gt;
&lt;h1&gt;2. Society is constantly changing and so is "bias"&lt;/h1&gt;
&lt;p&gt;Ideally, discussions about norms and standards for a particular AI application would be resolved before development begins. But what counts as unfair or harmful behavior isn't stable—these factors evolve as societal debates progress, making a definitive solution to bias impossible. Worse, new biases can emerge if our AI systems don't adjust to these changes (&lt;a href="https://doi.org/10.1145/230538.230561"&gt;Friedman and Nissenbaum, 1996&lt;/a&gt;; &lt;a href="https://dl.acm.org/doi/10.1145/3442188.3445922"&gt;Bender et al., 2021&lt;/a&gt;). This concern is especially relevant for very large language models, which are expensive to train and therefore reused for many downstream tasks (&lt;a href="https://dl.acm.org/doi/10.1145/3442188.3445922"&gt;Bender et al., 2021&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Moreover, given the diverse applications that could use language technology, no single set of standards can fit them all. However, we can be transparent and detailed about how a particular model is trained, including dataset information, making this available for model transfer scenarios (&lt;a href="https://doi.org/10.1145/3287560.3287596"&gt;Mitchell et al., 2019&lt;/a&gt;; &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.463"&gt;Bender and Koller, 2020&lt;/a&gt;; &lt;a href="https://dl.acm.org/doi/fullHtml/10.1145/3458723?casa_token=TU9HFtTyB88AAAAA:J97L_b3qNCzO-r8MLt2yVnE9D6P4LwXsbuJDlhbUfYM1aTVT9oduXJq-reWE_zWaFw5DtrQyl-gH"&gt;Gebru et al., 2021&lt;/a&gt;; &lt;a href="https://dl.acm.org/doi/10.1145/3442188.3445922"&gt;Bender et al., 2021&lt;/a&gt;). Furthermore, we need technologies that allow us to counteract biases whenever they matter for downstream tasks. For this, we need a clear understanding of how bias originates in the first place.&lt;/p&gt;
&lt;h1&gt;3. Algorithmic bias is not only reflecting pre-existing bias&lt;/h1&gt;
&lt;p&gt;A popular argument in the AI community is that deep neural model bias simply reflects pre-existing biases in training data. However, we shouldn't neglect our responsibility in designing and implementing these systems: Many forms of bias can emerge at different stages of creating and deploying language technology (see &lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432"&gt;Hovy and Prabhumoye, 2021&lt;/a&gt;). In &lt;a href="https://arxiv.org/abs/2207.10245"&gt;Van der Wal et al., 2022&lt;/a&gt; we show how models may amplify biases seen in the dataset. Others have pointed out that biased algorithms can transform society in profound ways. For example, &lt;a href="https://proceedings.mlr.press/v81/ensign18a.html"&gt;Ensign et al. (2018)&lt;/a&gt; show how biased policing algorithms could result in increased surveillance of certain neighborhoods, which then feeds back into new data reinforcing the earlier bias, creating a 'runaway feedback loop'.&lt;/p&gt;
&lt;p&gt;Language technology doesn't merely reflect society—its implementations become part of society and can reshape it in unexpected ways. A well-known theme in the &lt;a href="https://www.futurelearn.com/info/courses/philosophy-of-technology/0/steps/26311"&gt;philosophy of technology&lt;/a&gt; is that technologies 'mediate' our experiences and shape our worldview of "how to live" (&lt;a href="https://www.psupress.org/books/titles/0-271-02539-5.html"&gt;Verbeek, 2005&lt;/a&gt;). Machine translation systems may promote a worldview primarily of men, with women restricted to stereotypical occupations (&lt;a href="https://www.humanamente.eu/index.php/HM/article/view/307"&gt;Wellner, 2020&lt;/a&gt;), and search engines showing only men for "CEO" searches similarly shape our perception of the archetypal business leader. Consider this example from &lt;a href="https://unesdoc.unesco.org/ark:/48223/pf0000367823"&gt;this UNESCO/COMEST report from 2019&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"The 'gendering' of digital assistants, for example, may reinforce understandings of women as subservient and compliant. Indeed, female voices are routinely chosen as personal assistance bots, mainly fulfilling customer service duties, whilst the majority of bots in professional services such as the law and finance sectors, for example, are coded as male voices. This has educational implications with regards to how we understand 'male' vs 'female' competences, and how we define authoritative versus subservient positions."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How we define and measure bias may also influence how we conceptualize bias itself. In fairness metrics discussions, Jacobs and Wallach (&lt;a href="https://doi.org/10.1145/3442188.3445901"&gt;2021&lt;/a&gt;) highlight 'consequential validity'—the fact that "measurements shape the ways that we understand the construct itself"—which is often overlooked when designing bias metrics.&lt;/p&gt;
&lt;p&gt;How we define and measure racial and gender categorizations shapes how we view and act on these constructs in society; viewing gender as binary may harm non-binary communities (&lt;a href="https://papers.ssrn.com/abstract=3189696"&gt;Costanza-Chock, 2018&lt;/a&gt;). (And see &lt;a href="https://books.google.nl/books?hl=nl&amp;amp;lr=&amp;amp;id=BguXDwAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PP1&amp;amp;dq=What+Is+Race%3F+glasgow&amp;amp;ots=sdLkgr5WNm&amp;amp;sig=le0lOZEXn-WqjaEAyq1E2EHCoqI#v=onepage&amp;amp;q=What%20Is%20Race%3F%20glasgow&amp;amp;f=false"&gt;Glasgow, 2019&lt;/a&gt; for a discussion of different perspectives on &lt;em&gt;race&lt;/em&gt;.)&lt;/p&gt;
&lt;p&gt;Algorithmic bias is inherently complex due to its sociotechnical and context-sensitive nature, making precise definition difficult—yet discussions about its conceptualization are crucial for research (e.g., &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.485"&gt;Blodgett et al., 2020&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2211.13709"&gt;Van der Wal et al. 2024&lt;/a&gt;). Researchers cannot rely on a 'catch-all' bias metric, and mitigating harms might require more than simply removing biased information (&lt;a href="https://aclanthology.org/2022.bigscience-1.3"&gt;Talat et al., 2022&lt;/a&gt;). Is completely debiasing an AI system even possible? (See &lt;a href="https://arxiv.org/abs/2101.11974"&gt;Talat et al., 2021&lt;/a&gt; for a discussion of &lt;em&gt;debiasing&lt;/em&gt;.)&lt;/p&gt;
&lt;h1&gt;Looking Forward&lt;/h1&gt;
&lt;p&gt;Perhaps our starting point should not be how to debias AI models, but rather to focus on larger societal questions: How do we want to shape the world with language technology as part of life? How can we design AI systems that help create a more just society, instead of reinforcing or creating new forms of systemic bias?&lt;/p&gt;
&lt;p&gt;Such broad discussions about fair behavior in AI systems need to involve not only AI researchers but various experts from outside the technical domain. By expanding the conversation beyond technical solutions to encompass ethical, social, and philosophical dimensions, we can work toward language technologies that truly serve the diverse needs of our changing society.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Thanks to &lt;a href="https://staff.fnwi.uva.nl/w.c.moltmaker/"&gt;Wout Moltmaker&lt;/a&gt; for his helpful comments on this blog post.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>The Birth of Bias: A case study on the evolution of gender bias in an English language model</title><link href="https://odvanderwal.nl/2022/birth-of-bias.html" rel="alternate"></link><published>2022-07-14T00:00:00+02:00</published><updated>2022-07-14T00:00:00+02:00</updated><author><name>Oskar van der Wal</name></author><id>tag:odvanderwal.nl,2022-07-14:/2022/birth-of-bias.html</id><summary type="html">&lt;p&gt;The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on neural networks and trained on vast amounts of training …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on neural networks and trained on vast amounts of training data, which make these so effective, but this doesn't come without any problems.&lt;/p&gt;
&lt;p&gt;LMs have also been shown to learn undesirable biases towards certain
social groups, which may unfairly influence the decisions, recommendations or texts
that AI systems building on those LMs generate. Research on detecting such biases is
crucial, but as new LMs are continuously developed, it is equally important to study
how LMs come to be biased in the first place, and what role the training data, architecture, and downstream application play at various phases in the life-cycle of an NLP model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this blog post, we outline our work towards a better understanding of the origins of bias in LMs by investigating the learning dynamics of an LSTM-based language model trained on an English Wikipedia corpus.&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;Gender bias for occupations in LMs&lt;/h1&gt;
&lt;p&gt;In our research, we studied a relatively small LM based on the LSTM architecture &lt;a href="https://arxiv.org/abs/1803.11138" title="K. Gulordava et al. “Colorless Green Recurrent Networks Dream Hierarchically”. 2018."&gt;[1]&lt;/a&gt; and only consider one of the most studied types of bias in language models: gender bias. Because of this constrained focus, we were able to leverage previous work on measuring gender bias and get a detailed view of how it is learnt from the start of training, which wouldn't be feasible with a very large pre-trained LM.&lt;/p&gt;
&lt;p&gt;In the image below you see a typical language modelling pipeline. As you can see there are multiple stages where the gender bias of a word can be measured: (&lt;strong&gt;I&lt;/strong&gt;) in the (training) dataset of the model, (&lt;strong&gt;II&lt;/strong&gt;) in the input embeddings (IE), which comprise the first layer of the LM, or (&lt;strong&gt;III&lt;/strong&gt;) at the end of the pipeline in a downstream task that makes use of the contextual embeddings of the model.&lt;/p&gt;
&lt;p&gt;&lt;img alt="c685cc03a9e1997cf52e804659c48055.png" src="https://odvanderwal.nl/images/c685cc03a9e1997cf52e804659c48055.png"&gt;&lt;/p&gt;
&lt;p&gt;At each of these stages, we measure the gender bias of the LM. Defining (undesirable) bias is a complicated matter, however, and there are many definitions and frameworks discussed in the literature [e.g. &lt;a href="https://aclanthology.org/P19-1159.pdf" title="T. Sun et al. “Mitigating Gender Bias in Natural Language Processing: Literature Review”. 2019."&gt;2&lt;/a&gt;,&lt;a href="https://aclanthology.org/2020.acl-main.468/" title="D. Shah et al. “Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview”. 2020."&gt;3&lt;/a&gt;,&lt;a href="https://aclanthology.org/2020.acl-main.485/" title="S. L. Blodgett et al. “Language (Technology) is Power: A Critical Survey of ”Bias” in NLP”. 2020."&gt;4&lt;/a&gt;]. In our work, we used gender-neutrality as the norm, and define bias as any significant deviation from a 50-50% distribution in preferences, probabilities or similarities. &lt;/p&gt;
&lt;p&gt;Inspired by previous work [&lt;a href="https://www.science.org/doi/abs/10.1126/science.aal4230" title="A. Caliskan et al. “Semantics derived automatically from language corpora contain human-like biases”. 2017."&gt;5&lt;/a&gt;,&lt;a href="https://aclanthology.org/N18-2002.pdf" title="R. Rudinger et al. “Gender Bias in Coreference Resolution”. 2018."&gt;6&lt;/a&gt;,&lt;a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020."&gt;7&lt;/a&gt;], we then considered the gender bias of 54 occupations terms. To quantify gender bias of occupation words, we use a set of unambiguously gendered word-pairs (e.g. “man”-“woman”, “he”-“she”, “king”-“queen”) in our measures. It is important to keep in mind, however, that ‘gender’ is a multifaceted concept, which is much more complicated than a simple male-female dichotomy suggests [&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0891243287001002002" title="C. West and D. H. Zimmerman. “Doing gender”. 1987"&gt;8&lt;/a&gt;,&lt;a href="https://www.tandfonline.com/doi/full/10.3109/09540261.2015.1106446" title="C. Richards et al. “Non-binary or genderqueer genders”. 2016."&gt;9&lt;/a&gt;].&lt;/p&gt;
&lt;p&gt;Now that we have discussed what we mean with gender bias, we'll turn to what we found in our experiments.&lt;/p&gt;
&lt;h1&gt;The evolution of gender representation in the input embeddings&lt;/h1&gt;
&lt;p&gt;Before we studied the gender bias for occupation terms, we first focused on the question: &lt;em&gt;How does the LM learn a representation of gender during training, and how is that representation encoded in the input embeddings?&lt;/em&gt; To answer this question, we trained several linear classifiers to predict the gender of 82 gendered word pairs (e.g. "he"-"she", "son"-"daughter", etc.). We did this for several snapshots during training time, one after each epoch (one pass over all the training data) and multiple during the first epoch. The LM was fully trained after 40 epochs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_decision_boundary.png" src="https://odvanderwal.nl/images/embedding_space_decision_boundary.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How good is this classifier if we give it all the axes of the input embedding space? And what if we only give it the single best one (which we call the &lt;/em&gt;&lt;em&gt;gender unit&lt;/em&gt;&lt;em&gt;)?&lt;/em&gt; Interestingly, we find that after around 2 epochs of training, only using the gender unit as input outperforms using all the other axes of the input embedding space! It seems that the information on the gender is encoded highly locally in the input embeddings early on during training. &lt;/p&gt;
&lt;p&gt;&lt;img alt="b0b3886708e79ac4530ae12296f8a358.png" src="https://odvanderwal.nl/images/b0b3886708e79ac4530ae12296f8a358.png"&gt;&lt;/p&gt;
&lt;p&gt;Another interesting finding, is that the dominant &lt;strong&gt;gender unit&lt;/strong&gt; seems to be specialized for marking female words in particular, while information on the male words is distributed over the rest of the input embedding space. In the animation below, the top row contains the correct predictions for the gender unit classifier, while the right column the correct predictions for the classifier trained on all the other axes as input. The distance from the origin indicates how far the word is from the respective decision boundaries. Note how the for the gender unit, words like "she", "her", "herself", and "feminine" are quickly moving to the top, while the animation ends with masculine words are only predicted correctly by the classifier with the gender unit removed.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ezgif-7-ef259504a3.gif" src="https://odvanderwal.nl/images/ezgif-7-ef259504a3.gif"&gt;&lt;/p&gt;
&lt;h1&gt;The evolution of gender bias&lt;/h1&gt;
&lt;p&gt;Using a linear decision boundary for studying how gender is represented in an embedding space---like we did above---is actually very similar to one class of (gender) bias measures: those based on finding a linear &lt;strong&gt;gender subspace&lt;/strong&gt;. Following Ravfogel, we define the gender subspace as the axis orthogonal to the decision boundary of a linear SVM that is trained to predict the gender for a set of male and female words &lt;a href="https://aclanthology.org/2020.acl-main.647/" title="S. Ravfogel et al. “Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection”. 2020."&gt;[10]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_gender_subspace.png" src="https://odvanderwal.nl/images/embedding_space_gender_subspace.png"&gt;&lt;/p&gt;
&lt;p&gt;Then, for finding the bias score of a word in the embedding space, you can look at the scalar projection on the gender subspace. In the example below, words like "engineer" and "scientist" are on the "male" side of the subspace (to the left of the purple decision boundary), while "nurse" and "receptionist" have a female bias (to the right). The farther away from the decision boundary, the higher the bias.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_occupation_bias.png" src="https://odvanderwal.nl/images/embedding_space_occupation_bias.png"&gt;&lt;/p&gt;
&lt;p&gt;Okay, so we have a measure for the gender bias in the input embeddings of the LM. However, as you may remember from the language modelling pipeline figure at the start of the blog post, this is only one part of the model. It is also important to consider the gender bias in a downstream task, as it the bias here that is very likely to actually harm people (the input embeddings are hidden from the users). Unfortunately, it is not clear how the bias in the internal state of the model, such as the input embeddings, is related to its behaviour downstream. &lt;/p&gt;
&lt;p&gt;For this reason, we also measure the gender bias in a downstream task, in particular the &lt;em&gt;semantic textual similarity task for bias&lt;/em&gt; (&lt;strong&gt;STS-B&lt;/strong&gt;) &lt;a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020."&gt;[11]&lt;/a&gt;. In this task, the LM is used for estimating the similarity of three sentences, containing either the word "man", "woman", or an occupation term. Then the gender bias for that occupation term is the difference in the similarity averaged over 276 template sentences. Below you see an example for finding the gender bias for "janitor" using one template sentence, where the final score is 0.75 - 0.54 = 0.21 (male bias).&lt;/p&gt;
&lt;p&gt;&lt;img alt="STS-B.png" src="https://odvanderwal.nl/images/STS-B.png"&gt;&lt;/p&gt;
&lt;p&gt;Equiped with these tools for measuring bias, we studied how the gender bias in the input embeddings and the STS-B task change during training. In our work, we have found a strinking gender bias in both representations (which even correlates with the percentage of male versus female workers in official US Labour Statistics &lt;a href="https://www.science.org/doi/abs/10.1126/science.aal4230" title="A. Caliskan et al. “Semantics derived automatically from language corpora contain human-like biases”. 2017."&gt;[5]&lt;/a&gt;). &lt;/p&gt;
&lt;p&gt;In the figure below, you can find the bias scores in the LM during three points in time. We find that the bias measured in the input embeddings is fairly predictive of the bias for the occupation words in the STS-B task, although the correlation between the two measures is not higher than 0.6 at the end of the training, indicating that there are still some important differences. One interesting difference between the two measures, is that we can find a gender asymmetry in the input embedding bias (skewed towards a female bias at epoch 40), but this asymmetry seems to be masked at the level of the downstream task.&lt;/p&gt;
&lt;p&gt;&lt;img alt="f32e52d4a61f0785298db563c2c5d8ca.png" src="https://odvanderwal.nl/images/f32e52d4a61f0785298db563c2c5d8ca.png"&gt;&lt;/p&gt;
&lt;h1&gt;Diagnostic intervention: changing downstream bias by changing embeddings&lt;/h1&gt;
&lt;p&gt;In discussing the bias scores above, we have found that the bias in the input embeddings does in fact correlate with the STS-B bias. However, it is not clear whether there is also a &lt;em&gt;causal&lt;/em&gt; relationship between the two representations. For this reason, we also did a diagnostic intervention: &lt;em&gt;does debiasing the input embeddings result in a measurable effect on the bias measured downstream.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Again, we use a method proposed by Ravfogel to debias the input embeddings &lt;a href="https://aclanthology.org/2020.acl-main.647/" title="S. Ravfogel et al. “Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection”. 2020."&gt;[10]&lt;/a&gt;. To do so, we simply project all the words on the null-space of the gender subspace (which is the decision boundary), effectively removing the gender information from this subspace. However, as Ravfogel has noted, there may be several linear representations and therefore this procedure---of finding a subspace and debiasing---should be repeated multiple times. We indicate the numer of debiasing steps with &lt;strong&gt;k&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_debiased.png" src="https://odvanderwal.nl/images/embedding_space_debiased.png"&gt;&lt;/p&gt;
&lt;p&gt;To study the effect of debiasing the input embeddings on the bias in the downstream STS-B task, we debias 10 times. For each step &lt;strong&gt;k&lt;/strong&gt;, we then measure (i) the STS-B bias, (ii) the effect on the &lt;a href="https://en.wikipedia.org/wiki/Perplexity" title="An increase in the perplexity means that the LM becomes worse in its language modelling capabilities."&gt;perplexity&lt;/a&gt; of the LM, and (iii) &lt;a href="https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full" title="We used Representational Similarity Analysis (RSA) to see how much the topology of the embedding space has changed due to debiasing."&gt;the topological similarity&lt;/a&gt; of the embeddings compared to the original model. The perplexity values are normalized to one with respect to perplexity before debiasing.&lt;/p&gt;
&lt;p&gt;In the figure below, you see the results for the LM after 1 and 40 epochs of training. A few surprising observations can be made here. First, while the bias of the original LM at epoch 40 is higher, debiasing is much more effective at the end of the training and results in a lower bias in the end. This agrees with our earlier finding that gender information is represented more locally at the end of the training, meaning that the bias is easier to remove effectively and selectively. Secondly, while debiasing very often damages the LM considerably, doing this only for three times reduces the bias without many side-effects. Both the perplexity of the LM and the topological similarity of the input embeddings are very similar to situation before debiasing.&lt;/p&gt;
&lt;p&gt;&lt;img alt="8b3a882ec09f00a023c0ea0ee781eb95.png" src="https://odvanderwal.nl/images/8b3a882ec09f00a023c0ea0ee781eb95.png"&gt;&lt;/p&gt;
&lt;p&gt;Since we have observed some gender asymmetries in our earlier experiments, we also wanted to study the effect on the female and male biased occupations separately. In the figure below you can see the effect of debiasing on the input embedding bias (left) and STS-B bias (right). Interestingly, when consider the input embeddings, we see that debiasing once reduces especially the female bias (and even results in an increase of the measured male bias!). This could be explained by the fact that the dominant gender unit we identified earlier in this blogpost specializes in female word information. Strangely enough, for the STS-B bias we see that the first debiasing step reduces the male bias more, and more research is needed to explain this behaviour.&lt;/p&gt;
&lt;p&gt;&lt;img alt="e8240b159bf9bd220fc6a9ec11716371.png" src="https://odvanderwal.nl/images/e8240b159bf9bd220fc6a9ec11716371.png"&gt;&lt;/p&gt;
&lt;p&gt;While we have found that there is a causal relationship between the input embedding and STS-B bias, more work is needed on the precise nature of how they are connected. It is especially important to be aware of the possible (gender) asymmetries in the internal representations of the model, as naive mitigation strategies may introduce new undesirable biases by having a disparate effect on one social category.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We identify different phases in the learning dynamics of gender (bias): in how locally it is represented and the different dataset features that can explain these.&lt;/li&gt;
&lt;li&gt;We have seen that the concept of gender is encoded increasingly locally in the input embeddings during training, and that the dominant &lt;strong&gt;gender unit&lt;/strong&gt; specializes in feminine word information.&lt;/li&gt;
&lt;li&gt;The bias measured in the input embeddings are fairly predictive of the bias measured downstream in a simple task. We even found that debiasing the input embeddings is effective in reducing the downstream bias, indicating some causal relationship.&lt;/li&gt;
&lt;li&gt;We observed a striking gender asymmetry in our results, for instance, in how the dominant gender unit in the input embedding space specializes in female word information and how debiasing has a different effect depending on the gender bias of the word. This latter observation underlines the importance of understanding how the (gender) bias is represented in the internal state of the LM, as naive debiasing may introduce new biases.&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>PhD Tools: Zotero</title><link href="https://odvanderwal.nl/2022/phd-tools-zotero.html" rel="alternate"></link><published>2022-04-01T00:00:00+02:00</published><updated>2022-04-01T00:00:00+02:00</updated><author><name>Oskar van der Wal</name></author><id>tag:odvanderwal.nl,2022-04-01:/2022/phd-tools-zotero.html</id><summary type="html">&lt;p&gt;&lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; is a free and open source reference manager. I like how polished the software is and the many features/plugins that are available. With the &lt;a href="https://www.zotero.org/download/connectors"&gt;browser extension&lt;/a&gt; I can easily capture articles while browsing the internet, with group libraries I can organize and share papers with different research groups …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt; is a free and open source reference manager. I like how polished the software is and the many features/plugins that are available. With the &lt;a href="https://www.zotero.org/download/connectors"&gt;browser extension&lt;/a&gt; I can easily capture articles while browsing the internet, with group libraries I can organize and share papers with different research groups, and with the &lt;a href="https://retorque.re/zotero-better-bibtex/"&gt;Better BibTex&lt;/a&gt; plugin I can export my bibliography to a bibtex file.
Here I'll share some more tricks on using Zotero, which I thought would be useful for my colleagues at the University of Amsterdam.&lt;/p&gt;
&lt;h1&gt;SURFdrive as webdav backend for Zotero&lt;/h1&gt;
&lt;p&gt;Zotero provides a sync service so that you can synchronise all your papers across your different devices. However, the free storage is limited to 300 MB, and before you know it you're out of space.
Fortunately, you can also use a WebDav backend for your storage instead.
Here I'll show how you can use &lt;a href="https://surfdrive.surf.nl/"&gt;SURFdrive&lt;/a&gt; for your Zotero storage, where people at the University of Amsterdam get 500GB for free.&lt;/p&gt;
&lt;p&gt;After logging into SURFdrive, you need to go to your Settings, where you can also find your username (&lt;code&gt;something@uva.nl&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img alt="firefox_RXkDcjtLL3.png" src="https://odvanderwal.nl/images/firefox_RXkDcjtLL3.png"&gt;&lt;/p&gt;
&lt;p&gt;Then head to the &lt;code&gt;Security&lt;/code&gt; tab.&lt;/p&gt;
&lt;p&gt;&lt;img alt="firefox_qGN4Gve4KZ.png" src="https://odvanderwal.nl/images/firefox_qGN4Gve4KZ.png"&gt;&lt;/p&gt;
&lt;p&gt;Scroll down, and you can find the settings for creating an app password, which you will need for Zotero. Create an app password and take note of your password, which will be of the form: &lt;code&gt;XXXXX-XXXXX-XXXXX-XXXXX&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="firefox_udE0yEgLKC.png" src="https://odvanderwal.nl/images/firefox_udE0yEgLKC.png"&gt;&lt;/p&gt;
&lt;p&gt;The last thing you need to do in your SURFdrive, is to create a &lt;code&gt;zotero/&lt;/code&gt; folder somewhere. Depending on the location, you will have to change the WebDAV url, which is &lt;code&gt;https://surfdrive.surf.nl/files/remote.php/nonshib-webdav/&lt;/code&gt; + the location of your &lt;code&gt;zotero/&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;If your zotero folder is in the root of your SURFdrive, the WebDAV url becomes: &lt;code&gt;https://surfdrive.surf.nl/files/remote.php/nonshib-webdav/zotero/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now open Zotero and go to &lt;code&gt;Edit/Preferences&lt;/code&gt; and the tab &lt;code&gt;Sync&lt;/code&gt;. Select WebDAV for syncing your attachment files (i.e. the PDF files). Then fill in the WebDAV URL, SURFdrive Username, and your App Password. Finish by verifying the server settings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="zotero_RhRLI0o2Yw.png" src="https://odvanderwal.nl/images/zotero_RhRLI0o2Yw.png"&gt;&lt;/p&gt;
&lt;h1&gt;Mobile workflow&lt;/h1&gt;
&lt;p&gt;Recently, Zotero announced &lt;a href="https://www.zotero.org/support/ios"&gt;Zotero for iOS&lt;/a&gt;, which makes it easy to read and annotate your papers on an iPad.&lt;/p&gt;
&lt;p&gt;Before that, I made use of the &lt;a href="http://zotfile.com/"&gt;ZotFile&lt;/a&gt; plugin to select papers to be shared with other devices (syncing the shared folder using &lt;a href="https://syncthing.net/"&gt;Syncthing&lt;/a&gt;, but you can also use Dropbox). You can still use that workflow on Android, which has no official Zotero app. However, I have found the &lt;a href="https://play.google.com/store/apps/details?id=com.mickstarify.zooforzotero&amp;amp;gl=US"&gt;Zoo for Zotero&lt;/a&gt; android app to work really well in combination with the &lt;a href="https://play.google.com/store/apps/details?id=com.xodo.pdf.reader&amp;amp;gl=US"&gt;Xodo PDF Reader&lt;/a&gt; on Android.&lt;/p&gt;
&lt;p&gt;For both iOS and Android you can use the same WebDAV settings for accessing your Zotero papers.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>How do I know if my language model is gender-biased?</title><link href="https://odvanderwal.nl/2022/gender-bias-in-language-models.html" rel="alternate"></link><published>2022-02-02T00:00:00+01:00</published><updated>2022-02-02T00:00:00+01:00</updated><author><name>Oskar van der Wal</name></author><id>tag:odvanderwal.nl,2022-02-02:/2022/gender-bias-in-language-models.html</id><summary type="html">&lt;p&gt;&lt;em&gt;This blog post appeared on &lt;a href="https://bias-barometer.github.io/blogs_posts/gender-bias-in-language-models/"&gt;the Bias Barometer website&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This blog post appeared on &lt;a href="https://bias-barometer.github.io/blogs_posts/gender-bias-in-language-models/"&gt;the Bias Barometer website&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The language model (LM) is an essential building block of current AI systems dealing with natural language, and has proven to be useful in tasks as diverse as sentiment analysis, text generation, translations, and summarizing. These LMs are typically based on deep neural networks and trained on vast amounts of training data, which makes these so effective, but this doesn't come without any problems.&lt;/p&gt;
&lt;p&gt;LMs have been shown to learn undesirable biases towards certain
social groups, which may unfairly influence the decisions, recommendations or texts
that AI systems building on those LMs generate.
However, the black-box nature of deep neural networks and the fact that these are trained on very large datasets makes it difficult to understand how LMs are biased. Nevertheless, researchers have proposed many different techniques to study the biases that are present in current natural language technologies.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In this blog post, we will give some examples of techniques we use in our lab to measure one well-studied type of bias, gender bias, in the different representations of language models.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;What are language models?&lt;/h2&gt;
&lt;p&gt;A language model (LM) is a statistical model to predict the next likely word given a sequence of other words. The most successful ones are based on deep neural networks. There exist many different types of networks (e.g. LSTM, Transformer), but all rely on encoding words or word parts as vector representations and passing these through the "hidden layers" of the network resulting in "contextual embeddings" (called such because contextual information is represented in the vectors, such as information on the neighbouring words). Typically, LMs have billions of parameters that are trained on huge datasets, which makes these so effective but also very complex systems.&lt;/p&gt;
&lt;p&gt;While we have a lot of examples showing that LMs exhibit undesirable gender biases, it remains challenging to measure gender bias in these models. In this post, we will focus on three stages in a typical LM pipeline, as highlighted in the figure below, and give examples of how you can measure gender bias in these different representations.&lt;/p&gt;
&lt;p&gt;&lt;img alt="c685cc03a9e1997cf52e804659c48055.png" src="https://odvanderwal.nl/images/c685cc03a9e1997cf52e804659c48055.png"&gt;&lt;/p&gt;
&lt;p&gt;First, we discuss how gender bias can be measured in the training dataset of a model (I) and at the end of the pipeline in a downstream task that makes use of the contextual embeddings of the model (III). We then give an example of how to study gender bias in the internal state of the LM, by studying the input embeddings (IE), wich make up the first layer of the LM (II). However, before we discuss these methods, we give a brief explanation of what we mean by gender bias.&lt;/p&gt;
&lt;h2&gt;Gender bias of occupation terms&lt;/h2&gt;
&lt;p&gt;Defining (undesirable) bias is a complicated matter, and there are many definitions and frameworks discussed in the literature [e.g. &lt;a href="https://aclanthology.org/P19-1159.pdf" title="T. Sun et al. “Mitigating Gender Bias in Natural Language Processing: Literature Review”. 2019."&gt;1&lt;/a&gt;, &lt;a href="https://aclanthology.org/2020.acl-main.468/" title="D. Shah et al. “Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview”. 2020."&gt;2&lt;/a&gt;, &lt;a href="https://aclanthology.org/2020.acl-main.485/" title="S. L. Blodgett et al. “Language (Technology) is Power: A Critical Survey of ”Bias” in NLP”. 2020."&gt;3&lt;/a&gt;]. In this post, we use &lt;em&gt;gender-neutrality&lt;/em&gt; as the norm, and define bias as any significant deviation from a 50-50% distribution in preferences, probabilities or similarities. &lt;/p&gt;
&lt;p&gt;Inspired by previous work [&lt;a href="https://www.science.org/doi/abs/10.1126/science.aal4230" title="A. Caliskan et al. “Semantics derived automatically from language corpora contain human-like biases”. 2017."&gt;4&lt;/a&gt;, &lt;a href="https://aclanthology.org/N18-2002.pdf" title="R. Rudinger et al. “Gender Bias in Coreference Resolution”. 2018."&gt;5&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020."&gt;6&lt;/a&gt;], we consider the gender bias of occupation terms in the examples. To quantify the gender bias of an occupation, we often use unambiguously gendered word-pairs (e.g. “man”-“woman”, “he”-“she”, “king”-“queen”) in our measures. It is important to keep in mind, however, that ‘gender’ is a multifaceted concept, which is much more complicated than a simple male-female dichotomy suggests [&lt;a href="https://journals.sagepub.com/doi/abs/10.1177/0891243287001002002" title="C. West and D. H. Zimmerman. “Doing gender”. 1987"&gt;7&lt;/a&gt;, &lt;a href="https://www.tandfonline.com/doi/full/10.3109/09540261.2015.1106446" title="C. Richards et al. “Non-binary or genderqueer genders”. 2016."&gt;8&lt;/a&gt;].&lt;/p&gt;
&lt;h2&gt;Dataset bias&lt;/h2&gt;
&lt;p&gt;It is not a strange idea to investigate the gender bias in the training data of the LM. In the end, the model learns these biases from the data in some way or another. A typical approach to quantify the dataset bias, is to use measurable features in the dataset, such as word counts or how often gendered words co-occur with the words of interest [&lt;a href="https://ojs.aaai.org/index.php/ICWSM/article/view/14744" title="E. Fast et al. Shirtless and dangerous: Quantifying linguistic signals of gender bias in an online fiction writing community. 2016."&gt;10&lt;/a&gt;, &lt;a href="https://pile.eleuther.ai/paper.pdf" title="L. Gao et al. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. 2020."&gt;11&lt;/a&gt;, &lt;a href="https://proceedings.neurips.cc/paper/2019/file/201d546992726352471cfea6b0df0a48-Paper.pdf" title="Y.C. Tan et al. Assessing Social and Intersectional Biases in Contextualized Word Representations. 2019."&gt;12&lt;/a&gt;, &lt;a href="https://aclanthology.org/N19-1064.pdf" title="J. Zhao et al. Gender Bias in Contextualized Word Embeddings. 2019."&gt;13&lt;/a&gt;]. While these statistics can give some indication of possible sources of bias, it doesn't tell use about more nuanced and implicit ways that gender bias can be present in texts that may still be picked up by LMs. Other researchers use special trained classifiers for showing gender bias in texts [&lt;a href="https://dl.acm.org/doi/abs/10.1145/3287560.3287572" title="M. De-Artega et al. Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting. 2019."&gt;14&lt;/a&gt;, &lt;a href="https://aclanthology.org/2020.emnlp-main.23.pdf" title="E. Dinan et al. Multi-Dimensional Gender Bias Classification. 2020."&gt;15&lt;/a&gt;, &lt;a href="https://aclanthology.org/2020.emnlp-main.44.pdf" title="A. Field. Unsupervised discovery of implicit gender bias. 2020."&gt;16&lt;/a&gt;]. However, how these biases are learnt from texts by LMs and what features are important is still an active area of research. &lt;/p&gt;
&lt;h2&gt;Downstream bias&lt;/h2&gt;
&lt;p&gt;Most work on the bias of LMs is focused on the output. Any bias in the predictions, recommendations, and texts generated in the downstream task has the potential to actually harm people by unfair behaviour &lt;a href="https://aclanthology.org/2020.acl-main.485/" title="S.L. Blodgett et al. Language (technology) is power: A critical survey of \&amp;quot;bias\&amp;quot; in nlp. 2020."&gt;[17]&lt;/a&gt;. This bias is typically tested by &lt;em&gt;challenge sets&lt;/em&gt;: carefully constructed sets of sentences used to probe the model for any specific biases. &lt;/p&gt;
&lt;p&gt;One example of a challenge set is the &lt;em&gt;semantic textual similarity task for bias&lt;/em&gt; (&lt;strong&gt;STS-B&lt;/strong&gt;) &lt;a href="https://arxiv.org/abs/2010.06032" title="K. Webster et al. Measuring and Reducing Gendered Correlations in Pre-Trained Models. 2020."&gt;[18]&lt;/a&gt;. In this task, the LM is used for estimating the similarity of three sentences, containing either the word "man", "woman", or an occupation term. Then the gender bias for that occupation term is the difference in the similarity averaged over a set of template sentences. Below you see an example for finding the gender bias for "janitor" using one template sentence, where the final score is 0.75 - 0.54 = 0.21 (male bias).&lt;/p&gt;
&lt;p&gt;&lt;img alt="STS-B.png" src="https://odvanderwal.nl/images/STS-B.png"&gt;&lt;/p&gt;
&lt;h2&gt;Embedding bias&lt;/h2&gt;
&lt;p&gt;We have seen some examples of measuring gender bias in the training data of an LM and at its output in a downstream task. Something that is not studied as much, is the gender bias in the internal states of an LM. Because of the black-box nature of deep neural networks this is notoriously difficult. &lt;/p&gt;
&lt;p&gt;However, there is one layer in a typical LM that is actually very suitable for measuring bias: the input embeddings. The input embeddings are the first layer of the LM and encode word (parts) into word vectors, which can be used in the other layers. The input embeddings actually have a similar representation to &lt;em&gt;static word embeddings&lt;/em&gt;, for which researchers have actually developed many techniques to measure gender bias. One class of bias measures relies on finding a linear &lt;strong&gt;gender subspace&lt;/strong&gt;. Let us explain one of these methods from Ravfogel et al. &lt;a href="https://aclanthology.org/2020.acl-main.647/" title="S. Ravfogel et al. “Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection”. 2020."&gt;[19]&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;For this measure, we start with a set of unambiguously female and male words (e.g. "man"-"woman", "he"-"she", "son"-"daughter") and train a linear classifier, such as a &lt;em&gt;support vector machine&lt;/em&gt; (SVM), to predict the gender of each word.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_decision_boundary.png" src="https://odvanderwal.nl/images/embedding_space_decision_boundary.png"&gt;&lt;/p&gt;
&lt;p&gt;We then define the &lt;strong&gt;gender subspace&lt;/strong&gt; as the axis orthogonal to the decision boundary of the linear SVM that is trained to predict the gender for a set of male and female words.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_gender_subspace.png" src="https://odvanderwal.nl/images/embedding_space_gender_subspace.png"&gt;&lt;/p&gt;
&lt;p&gt;Now we can use the gender subspace for quantifying the gender bias of, for example, occupation terms. For finding the bias score of a word in the embedding space, we use the scalar projection on the gender subspace. In the example below, words like "engineer" and "scientist" are on the "male" side of the subspace (to the left of the purple decision boundary), while "nurse" and "receptionist" have a female bias (to the right). The farther away from the decision boundary the word is, the higher the bias.&lt;/p&gt;
&lt;p&gt;&lt;img alt="embedding_space_occupation_bias.png" src="https://odvanderwal.nl/images/embedding_space_occupation_bias.png"&gt;&lt;/p&gt;
&lt;p&gt;Let us give an example of what we find if we apply this bias measure to the input embeddings of a Dutch LM called BERTje &lt;a href="https://arxiv.org/abs/1912.09582" title="W. de Vries et al. BERTje: A Dutch BERT Model. 2019."&gt;[20]&lt;/a&gt;. When studying the gender bias for Dutch occupation terms, we find clear gender stereotypes in the top 5 male and female biased occupations, as can be seen in the table below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Female bias (top 5)&lt;/th&gt;
&lt;th&gt;Male bias (top 5)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;kinderopvang (child care)&lt;/td&gt;
&lt;td&gt;ingenieur (engineer)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;verpleegkundige (nurse)&lt;/td&gt;
&lt;td&gt;chauffeur&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;verzorger (caretaker)&lt;/td&gt;
&lt;td&gt;kunstenaar (artist)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;administratie (administration)&lt;/td&gt;
&lt;td&gt;auteur (author)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;schoonheidsspecialist (beauty specialist)&lt;/td&gt;
&lt;td&gt;bouwvakker (construction worker)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this blog post, we wanted to give some examples of how you can measure the gender bias of a language model. We showed some methods for three different representations in the language modelling pipeline. While it is difficult to measure the gender bias in the internal states of LMs, because of their "black box" nature, it is actually possible to use existing bias measures on one of the layers: &lt;em&gt;the input embeddings&lt;/em&gt;, for which we also give some examples.&lt;/p&gt;
&lt;p&gt;However, it is important to keep in mind that the research on how to measure bias in LMs is ongoing. Moreover, we also still need more research on how the different bias measures relate to each other to form a good understanding of the underlying mechanisms of bias and the validity of these measures.&lt;/p&gt;</content><category term="blog"></category></entry></feed>